


#Importar Bibliotecas
from keras.models import Sequential, load_model
from keras.layers import Activation, Dropout, Flatten, Dense, Rescaling, BatchNormalization, Conv2D, MaxPooling2D, SpatialDropout2D, AveragePooling2D
from keras.callbacks import ModelCheckpoint
from numpy import load
from tensorboard import version
from keras.preprocessing import image
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, accuracy_score

import csv
import os, datetime
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import sklearn.datasets
import gc


#Verificar versão do TF e ver GPU

print(tf.__version__)


print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

print("tensorboard.version.VERSION: ", version.VERSION)


directory='../Images/64x64'

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    directory,
    label_mode='int',
    seed=1337,
    image_size=(64,64),
    batch_size= None,
)


image_array = []
label_array = []

for element in train_ds:  

    image_array.append(np.asarray(element[0]))
    label_array.append(np.asarray(element[1]))

image_array = np.asarray(image_array)
label_array = np.asarray(label_array)

print(image_array.shape)
print(label_array.shape)


x_train, x_test, y_train, y_test = train_test_split(image_array, label_array, test_size=0.1, random_state=42)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

 
# ## MODELO


gc.collect()

# Define per-fold score containers
acc_eval_per_fold = []
loss_eval_per_fold = []
acc_per_fold = []
loss_per_fold = []
precision_per_fold = []
recall_per_fold = []
f1_score_per_fold = []
cm_score_per_fold = []
history_cv = []

# Define the K-fold Cross Validator
kfold = KFold(n_splits=10, shuffle=True)

# K-fold Cross Validation model evaluation
fold_no = 1
with tf.device("/gpu:0"):
    for train, test in kfold.split(x_train,y_train):
                
        model = Sequential()
        ##normalizaçao [-1,1]
        model.add(Rescaling(1./127.5, offset=-1, input_shape=(64, 64, 3)))
        model.add(Conv2D(filters=128, kernel_size=(2), strides=(1), activation='tanh'))
        model.add(MaxPooling2D(pool_size=(2), padding='valid', strides=(1)))
        model.add(Flatten())
        model.add(Dense(15, activation='tanh'))
        model.add(Dense(5,activation='softmax'))
        
        model.compile(
            optimizer='adam',
            loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),
            metrics=['accuracy'])

        #Saved models path
        model_name = 'conv2D_crossVal_64x64'
        save_path_best_model = '../Models/CrossValidation/Conv2D_64x64/' + model_name + '_fold_' + str(fold_no) +'.hdf5'

        # CREATE CALLBACKS
        checkpoint_val_acc = tf.keras.callbacks.ModelCheckpoint(save_path_best_model, 
                                                        monitor='val_accuracy', verbose=0, 
                                                        save_best_only=True, mode='max')

        callbacks_list = [checkpoint_val_acc]

        # Generate a print
        print('--------------------------------------------------------------------------------')
        print(f'Training for fold {fold_no} ...')


        # Fit data to model - uses x_train to train and validate
        history = model.fit(x_train[train], y_train[train],validation_data=(x_train[test], y_train[test]), 
                            verbose=2, batch_size=80, callbacks=callbacks_list, epochs=50)

        gc.collect()
        history_cv.append(history)
        #load best model from fold
        best_model = load_model(save_path_best_model)

        # Generate generalization metrics - uses x_test to validate and predict
        scores = best_model.evaluate(x_train[test], y_train[test], verbose=0)

        # Calculate predicted labels for the test set
        y_pred = np.argmax(best_model.predict(x_test), axis=-1)

        # Calculate precision, recall, and F1 score
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='macro')
        recall = recall_score(y_test, y_pred, average='macro')
        f1 = f1_score(y_test, y_pred, average='macro')
        cm = confusion_matrix(y_test, y_pred)

        print(f'Score for fold {fold_no}: {best_model.metrics_names[0]} of {round(scores[0],5)}; {best_model.metrics_names[1]} of {round(scores[1]*100,5)}%')

        # Append scores to the corresponding lists
        acc_eval_per_fold.append(scores[1] * 100)
        loss_eval_per_fold.append(scores[0])

        acc_per_fold.append(accuracy*100)
        precision_per_fold.append(precision*100)
        recall_per_fold.append(recall*100)
        f1_score_per_fold.append(f1*100)
        cm_score_per_fold.append(cm)

        # Increase fold number
        fold_no = fold_no + 1

        gc.collect()


# Print the precision, recall, and F1 score for each fold, as well as the average scores
print('Score per fold')
for i in range(len(acc_eval_per_fold)):
    print('------------------------------------------------------------------------')
    print(f'> Fold {i+1} -  Eval Loss: {round(loss_eval_per_fold[i],5)} - Eval Accuracy: {round(acc_eval_per_fold[i],5)}% - Accuracy: {round(acc_per_fold[i],5)}% - Precision: {round(precision_per_fold[i],5)}% - Recall: {round(recall_per_fold[i],5)}% - F1 Score: {round(f1_score_per_fold[i],5)}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Evaluation Accuracy: {round(np.mean(acc_eval_per_fold),5)} (+- {round(np.std(acc_per_fold),5)})')
print(f'> Evaluation Loss: {round(np.mean(loss_eval_per_fold),5)}')
print(f'> Accuracy: {round(np.mean(acc_per_fold),5)} (+- {round(np.std(acc_per_fold),5)})')
print(f'> Precision: {round(np.mean(precision_per_fold),5)} (+- {round(np.std(precision_per_fold),5)})')
print(f'> Recall: {round(np.mean(recall_per_fold),5)} (+- {round(np.std(recall_per_fold),5)})')
print(f'> F1 Score: {round(np.mean(f1_score_per_fold),5)} (+- {round(np.std(f1_score_per_fold),5)})')
print('------------------------------------------------------------------------')


 
# ## GRAFICOS TEM DE SER FEITOS MANUALMENTE


csv_file_path = "conv2d_crossval_tanh_history_model_dense_15.csv"
with open(csv_file_path, mode='w', newline='') as file:
    writer = csv.writer(file)
    
    # Write the header
    writer.writerow(['fold', 'epoch', 'train_accuracy', 'train_loss', 'val_accuracy', 'val_loss'])
    
    # Iterate over all folds
    for i in range(10):
        fold = i + 1  # fold number
        train_acc = history_cv[i].history['accuracy']
        train_loss = history_cv[i].history['loss']
        val_acc = history_cv[i].history['val_accuracy']
        val_loss = history_cv[i].history['val_loss']
        
        # Write the data for each epoch in the fold
        for epoch in range(len(train_acc)):
            writer.writerow([fold, epoch + 1, train_acc[epoch], train_loss[epoch], val_acc[epoch], val_loss[epoch]])

print("History data for all folds saved to:", csv_file_path)
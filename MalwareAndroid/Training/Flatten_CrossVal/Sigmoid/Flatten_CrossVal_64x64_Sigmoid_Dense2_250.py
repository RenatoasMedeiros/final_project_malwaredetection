 


#Importar Bibliotecas
from keras.models import Sequential, load_model
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense, Rescaling, BatchNormalization
from keras.callbacks import ModelCheckpoint
from numpy import load
from tensorboard import version
import keras.backend as K
import csv
import os, datetime
import numpy as np
from keras.preprocessing import image

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, accuracy_score
from sklearn.model_selection import KFold

import sklearn.datasets
import gc


#Verificar versão do TF e ver GPU
import tensorflow as tf
print(tf.__version__)


print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

print("tensorboard.version.VERSION: ", version.VERSION)


directory='../Images/64x64'

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    directory,
    label_mode='int',
    seed=1337,
    image_size=(64,64),
    batch_size= None,
)


image_array = []
label_array = []

for element in train_ds:  
    
    image_array.append(np.asarray(element[0]))
    label_array.append(np.asarray(element[1]))

image_array = np.asarray(image_array)
label_array = np.asarray(label_array)

print(image_array.shape)
print(label_array.shape)


x_train, x_test, y_train, y_test = train_test_split(image_array, label_array, test_size=0.1, random_state=42)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

 
# ## MODELO 10


gc.collect()

# Define per-fold score containers
acc_eval_per_fold = []
loss_eval_per_fold = []
acc_per_fold = []
loss_per_fold = []
precision_per_fold = []
recall_per_fold = []
f1_score_per_fold = []
cm_score_per_fold = []
history_cv = []

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True  # Allow GPU memory allocation to grow as needed
config.gpu_options.per_process_gpu_memory_fraction = 0.8  # Adjust the value as needed
tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))

with tf.device("/gpu:0"):
    # Define the K-fold Cross Validator
    kfold = KFold(n_splits=10, shuffle=True)

    # K-fold Cross Validation model evaluation
    fold_no = 1
    for train, test in kfold.split(x_train,y_train):

        model10 = Sequential()
        ##normalizaçao [-1,1]
        model10.add(Rescaling(1./127.5, offset=-1, input_shape=(64, 64, 3)))
        model10.add(Flatten())
        model10.add(Dense(1350, activation='sigmoid'))
        model10.add(BatchNormalization())
        model10.add(Dropout(rate=0.85))
        model10.add(Dense(250, activation='sigmoid'))
        model10.add(BatchNormalization())
        model10.add(Dropout(rate=0.7))
        model10.add(Dense(5,activation='softmax'))

        model10.compile(
            optimizer='adam',
            loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),
            metrics=['accuracy'])

        #Saved models path
        model_name = 'flatten_crossval_64x64'
        
        save_path_best_model = '../Models/CrossValidation/Flatten_64x64/' + model_name + '_fold_' + str(fold_no) +'.hdf5'

        # CREATE CALLBACKS
        checkpoint_val_acc = tf.keras.callbacks.ModelCheckpoint(save_path_best_model, 
                                                        monitor='val_accuracy', verbose=0, 
                                                        save_best_only=True, mode='max')

        callbacks_list = [checkpoint_val_acc]

        # Generate a print
        print('--------------------------------------------------------------------------------')
        print(f'Training for fold {fold_no} ...')


        # Fit data to model - uses x_train to train and validate
        history10 = model10.fit(x_train[train], y_train[train],validation_data=(x_train[test], y_train[test]),
                                verbose=2, batch_size=128,callbacks=callbacks_list, epochs=50)

        gc.collect()

        history_cv.append(history10)

        #load best model from fold
        best_model = load_model(save_path_best_model)

        # Generate generalization metrics - uses x_test to validate and predict
        scores = best_model.evaluate(x_train[test], y_train[test], verbose=0)

        # Calculate predicted labels for the test set
        y_pred = np.argmax(best_model.predict(x_test), axis=-1)

        # Calculate precision, recall, and F1 score
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='macro')
        recall = recall_score(y_test, y_pred, average='macro')
        f1 = f1_score(y_test, y_pred, average='macro')
        cm = confusion_matrix(y_test, y_pred)

        print(f'Score for fold {fold_no}: {best_model.metrics_names[0]} of {round(scores[0],5)}; {best_model.metrics_names[1]} of {round(scores[1]*100,5)}%')

        # Append scores to the corresponding lists
        acc_eval_per_fold.append(scores[1] * 100)
        loss_eval_per_fold.append(scores[0])

        acc_per_fold.append(accuracy*100)
        precision_per_fold.append(precision*100)
        recall_per_fold.append(recall*100)
        f1_score_per_fold.append(f1*100)
        cm_score_per_fold.append(cm)

        # Increase fold number
        fold_no = fold_no + 1

        gc.collect()
        # Clear the TensorFlow session
        K.clear_session() 
    


# Print the precision, recall, and F1 score for each fold, as well as the average scores
print('Score per fold')
for i in range(len(acc_eval_per_fold)):
    print('------------------------------------------------------------------------')
    print(f'> Fold {i+1} -  Eval Loss: {round(loss_eval_per_fold[i],5)} - Eval Accuracy: {round(acc_eval_per_fold[i],5)}% - Accuracy: {round(acc_per_fold[i],5)}% - Precision: {round(precision_per_fold[i],5)}% - Recall: {round(recall_per_fold[i],5)}% - F1 Score: {round(f1_score_per_fold[i],5)}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Evaluation Accuracy: {round(np.mean(acc_eval_per_fold),5)} (+- {round(np.std(acc_per_fold),5)})')
print(f'> Evaluation Loss: {round(np.mean(loss_eval_per_fold),5)}')
print(f'> Accuracy: {round(np.mean(acc_per_fold),5)} (+- {round(np.std(acc_per_fold),5)})')
print(f'> Precision: {round(np.mean(precision_per_fold),5)} (+- {round(np.std(precision_per_fold),5)})')
print(f'> Recall: {round(np.mean(recall_per_fold),5)} (+- {round(np.std(recall_per_fold),5)})')
print(f'> F1 Score: {round(np.mean(f1_score_per_fold),5)} (+- {round(np.std(f1_score_per_fold),5)})')
print('------------------------------------------------------------------------')

    

csv_file_path = "flatten_crossval_sigmoid_model_history_dense2_250.csv"

with open(csv_file_path, mode='w', newline='') as file:
    writer = csv.writer(file)
    
    # Write the header
    writer.writerow(['fold', 'epoch', 'train_accuracy', 'train_loss', 'val_accuracy', 'val_loss'])
    
    # Iterate over all folds
    for i in range(10):
        fold = i + 1  # fold number
        train_acc = history_cv[i].history['accuracy']
        train_loss = history_cv[i].history['loss']
        val_acc = history_cv[i].history['val_accuracy']
        val_loss = history_cv[i].history['val_loss']
        
        # Write the data for each epoch in the fold
        for epoch in range(len(train_acc)):
            writer.writerow([fold, epoch + 1, train_acc[epoch], train_loss[epoch], val_acc[epoch], val_loss[epoch]])

print("History data for all folds saved to:", csv_file_path)




#Importar Bibliotecas
from keras.models import Sequential, load_model
from keras.layers import Activation, Dropout, Flatten, Dense, Rescaling, BatchNormalization, Conv2D, MaxPooling2D, SpatialDropout2D, AveragePooling2D
from keras.callbacks import ModelCheckpoint
#para limpeza da sessão
import keras.backend as K
from numpy import load
from tensorboard import version
from keras.preprocessing import image
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, accuracy_score

import csv
import os, datetime
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import sklearn.datasets
import gc


#Verificar versão do TF e ver GPU

print(tf.__version__)


print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

print("tensorboard.version.VERSION: ", version.VERSION)


directory='../Images/64x64'

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    directory,
    label_mode='int',
    seed=1337,
    image_size=(64,64),
    batch_size= None,
)


image_array = []
label_array = []

for element in train_ds:  

    image_array.append(np.asarray(element[0]))
    label_array.append(np.asarray(element[1]))

image_array = np.asarray(image_array)
label_array = np.asarray(label_array)

print(image_array.shape)
print(label_array.shape)


model_x_train, model_x_val, model_y_train, model_y_val = train_test_split(image_array, label_array,
                                                                          test_size=0.3, random_state=42)

model_x_val, model_x_test, model_y_val, model_y_test = train_test_split(model_x_val, model_y_val, test_size=0.5)

print('Data for training:')
print(model_x_train.shape)
print(model_y_train.shape)

print('Data for validation:')
print(model_x_val.shape)
print(model_y_val.shape)

print('Data for testing:')
print(model_x_test.shape)
print(model_y_test.shape)

 
# ## MODELO


gc.collect()

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True  # Allow GPU memory allocation to grow as needed
config.gpu_options.per_process_gpu_memory_fraction = 0.8  # Adjust the value as needed
tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))

with tf.device("/gpu:0"):
    print("Device used:", tf.test.gpu_device_name())
    model = Sequential()
    ##normalizaçao [-1,1]
    model.add(Rescaling(1./127.5, offset=-1, input_shape=(64, 64, 3)))
    model.add(Conv2D(filters=128, kernel_size=(2), strides=(1), activation='tanh'))
    model.add(MaxPooling2D(pool_size=(2), padding='valid', strides=(1)))
    model.add(Flatten())
    model.add(Dense(500, activation='tanh'))
    model.add(Dense(5,activation='softmax'))

    model.compile(
        optimizer='adam',
        loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),
        metrics=['accuracy'])

    #Saved models path
    model_name = 'conv2D_holdout_64x64'
    save_path_best_model = '../Models/Holdout/Conv2D_64x64/' + model_name + '.hdf5'

    # CREATE CALLBACKS
    checkpoint_val_acc = tf.keras.callbacks.ModelCheckpoint(save_path_best_model, 
                                                    monitor='val_accuracy', verbose=0, 
                                                    save_best_only=True, mode='max')

    callbacks_list = [checkpoint_val_acc]


    # Fit data to model - uses x_train to train and validate
    model_history = model.fit(model_x_train,model_y_train,validation_data=(model_x_val,model_y_val), 
                        verbose=2, batch_size=40, callbacks=callbacks_list, epochs=50)

    gc.collect()

    #load best model from fold
    best_model_holdout = load_model(save_path_best_model)

    # Generate generalization metrics - uses x_test to validate and predict
    model_scores = best_model_holdout.evaluate(model_x_val,model_y_val, verbose=0)

    # Calculate predicted labels for the test set
    model_y_pred = np.argmax(best_model_holdout.predict(model_x_test), axis=-1)

    # Calculate precision, recall, and F1 score
    model_accuracy = accuracy_score(model_y_test, model_y_pred)
    model_precision = precision_score(model_y_test, model_y_pred, average='macro')
    model_recall = recall_score(model_y_test, model_y_pred, average='macro')
    model_f1 = f1_score(model_y_test, model_y_pred, average='macro')
    model_cm = confusion_matrix(model_y_test, model_y_pred)
    
    # Clear the TensorFlow session
    K.clear_session()
    
    gc.collect()


print('\n----------------------------------------------------------')
print(f'Score: Eval {model.metrics_names[0]} of {round(model_scores[0],5)}; Eval {model.metrics_names[1]} of {round(model_scores[1]*100,5)}%')
print(f'> Accuracy: {round(model_accuracy*100,5)}%')
print(f'> Precision: {round(model_precision*100,5)}%')
print(f'> Recall: {round(model_recall*100,5)}%')
print(f'> F1 Score: {round(model_f1*100,5)}%')


train_acc = model_history.history['accuracy']
train_loss = model_history.history['loss']
val_acc = model_history.history['val_accuracy']
val_loss = model_history.history['val_loss']

# Define the file path
csv_file_path = "conv2d_holdout_tanh_history_model_dense_500.csv"

# Extract the history keys and values
history_data = {
    'epoch': range(1, len(train_acc) + 1),
    'train_accuracy': train_acc,
    'train_loss': train_loss,
    'val_accuracy': val_acc,
    'val_loss': val_loss
}

# Write the data to a CSV file
with open(csv_file_path, mode='w', newline='') as file:
    writer = csv.DictWriter(file, fieldnames=history_data.keys())
    writer.writeheader()
    for i in range(len(train_acc)):
        row_data = {key: history_data[key][i] for key in history_data.keys()}
        writer.writerow(row_data)

print("Model history data saved to:", csv_file_path)



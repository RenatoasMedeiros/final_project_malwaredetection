import os
import datetime
import matplotlib.pyplot as plt
import numpy as np
import csv
import keras_cv
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras import backend as K
from tensorflow import reshape
from keras.layers import BatchNormalization
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score
from keras.layers import Rescaling
from tensorflow import keras
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
from keras.applications import ResNet50
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau
from keras_cv import layers as kcv_layers

import time

K.clear_session()

#print("Keras version:", keras.__version__)
print("TensorFlow version:", tf.__version__)
print("keras_cv version:", keras_cv.__version__)


WMethod = 1 # Ver função getInitializer!
NLayer_ini = 5 # hidden layer Inicial  
NLayermax = 5 # Vai mostrar HLayermax-1 !
deltaNLayer = 1 # Aumento de hidden layer 
NEpochs = 50 # Numero de Epochs
n_folds = 5  # Numero de folds (>2)
N_neuronio = 64 # Numero de Neuronios
Batch_len = 40   # Tamanho do Batch Size
LearningRate = 0.001

Xinput = 4096 #64*64 # Input number
Youtput = 5  # Output number
Namostra= 10000  

directory='../Images/64x64'

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    directory,
    label_mode='int',
    seed=1337,
    image_size=(64,64),
    batch_size= None,
)

image_array = []
label_array = []

for element in train_ds:  
    
    image_array.append(np.asarray(element[0]))
    label_array.append(np.asarray(element[1]))

image_array = np.asarray(image_array)
label_array = np.asarray(label_array)

print(image_array.shape)
print(label_array.shape)

x_train, x_test, y_train, y_test = train_test_split(image_array, label_array, test_size=0.1, random_state=42)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)


class CustomCallback(Callback):
    def __init__(self, csv_file_path):
        super(CustomCallback, self).__init__()
        self.start_time = None
        self.batch_counter = 0
        self.csv_file_path = csv_file_path

    def on_train_begin(self, logs=None):
        self.start_time = time.time()

    def on_epoch_end(self, epoch, logs=None):
        with open(self.csv_file_path, mode='a', newline='') as file:
            writer = csv.writer(file)
            if os.path.getsize(self.csv_file_path) == 0:
                writer.writerow(['fold', 'epoch', 'train_accuracy', 'train_loss', 
                                            'val_accuracy', 'val_loss', 'epoch_time', 
                                            'val_precision', 'val_recall', 'val_f1' ])
            writer_csv = csv.writer(file)
          
            fold_no = 1  # Adjust this according to your fold number
            train_acc = logs.get('accuracy')
            train_loss = logs.get('loss')
            val_acc = logs.get('val_accuracy')
            val_loss = logs.get('val_loss')
            epoch_time = time.time() - self.start_time
            
            y_pred_val = np.argmax(self.model.predict(x_test), axis=-1)
            val_precision = precision_score(y_test, y_pred_val, average='macro')
            val_recall = recall_score(y_test, y_pred_val, average='macro')
            val_f1 = f1_score(y_test, y_pred_val, average='macro')
            
            writer_csv.writerow([fold_no, epoch + 1, train_acc, train_loss, 
                                val_acc, val_loss, epoch_time, val_precision,val_recall, val_f1])


model_to_train = ResNet50(include_top=False, weights=None, input_tensor=None,
                            input_shape=(64, 64, 3), pooling='Max')

# Manter a True para alterar os pesos
for layer in model_to_train.layers:
    layer.trainable = True

# Initialize the seeds within tensorflow for results to be reproducible
# Seeds for NumPy and TensorFlow
seed_value = 42
np.random.seed(seed_value)
tf.random.set_seed(seed_value)
# Settings for reproducibility (this may affect performance)
os.environ['TF_DETERMINISTIC_OPS'] = '2'

# Function that initialize the Weights of the Deep Learning Network for 15 different methods
def getInitialier(WMethod):
    if WMethod==1:
        print("Method GlorotNormal")
        iniM = tf.keras.initializers.GlorotNormal(seed=seed_value)
        return iniM, iniM, iniM
    if WMethod==2:
        print("Method GlorotUniform")
        iniM = tf.keras.initializers.GlorotUniform(seed=seed_value)
        return iniM, iniM, iniM
    if WMethod==3:
        print("Method HeNormal")
        iniM = tf.keras.initializers.HeNormal(seed=seed_value)
        return iniM, iniM, iniM
    if WMethod==4:
        print("Method HeUniform")
        iniM = tf.keras.initializers.HeUniform(seed=seed_value)
        return iniM, iniM, iniM
    if WMethod==5:
        print("Method LecunNormal")
        iniM = tf.keras.initializers.LecunNormal(seed=seed_value)
        return iniM, iniM, iniM
    if WMethod==6:
        print("Method LecunUniform")
        iniM = tf.keras.initializers.LecunUniform(seed=seed_value) 
        return iniM, iniM, iniM
    if WMethod==7:
        print("Method Orthogonal")
        iniM = tf.keras.initializers.Orthogonal()
        return iniM, iniM, iniM
    if WMethod==8:
        print("Method RandomNormal")
        iniM = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
        return iniM, iniM, iniM
    if WMethod==9:
        print("Method RandomUniform")
        iniM = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
        return iniM, iniM, iniM
    if WMethod==10:
        print("Method Identity")
        iniM = tf.keras.initializers.Identity()
        return iniM, iniM, iniM
    if WMethod==11:
        print("Method TruncatedNormal")
        iniM = tf.keras.initializers.TruncatedNormal(mean=0., stddev=0.5) 
        return iniM, iniM, iniM
    if WMethod==12:
        print("Method VarianceScaling")
        iniM = tf.keras.initializers.VarianceScaling(scale=0.1, mode='fan_in', distribution='uniform') 
        return iniM, iniM, iniM 
    if WMethod==13:
        print("Method Zeros")
        iniM = tf.keras.initializers.Zeros()
        return iniM, iniM, iniM 
    if WMethod==14:
        print("Method Ones")
        iniM = tf.keras.initializers.Ones()
        return iniM, iniM, iniM 


print("Ini Weight initialization")
initializer1, initializer2, initializer10 = getInitialier(WMethod)
print("End Weight initialization")


# Create empty arrays
HidenLayeres =[]
train_accuracies_=[]
losses_=[]
accuracies_=[]
val_losses_=[] 
precisions_=[]  
val_precisions_=[]      
recalles_=[]     
val_recalles_=[] 

# Define augmentation layers
# rotation_layer = kcv_layers.RandomRotation(factor=(-0.08, 0.08))
# shift_layer = kcv_layers.RandomTranslation(height_factor=0.08, width_factor=0.08)
# shear_layer = kcv_layers.RandomShear(x_factor=0.3, y_factor=0.3)
# zoom_layer = kcv_layers.RandomZoom(height_factor=0.02)

brightness_layer = kcv_layers.RandomBrightness(factor=0.2)  # Adjust brightness by a factor
#saturation_layer = kcv_layers.RandomSaturation(factor=0.2)  # Adjust saturation by a factor

# Combine augmentations into a pipeline
augmentation_pipeline = kcv_layers.RandomAugmentationPipeline([
    # rotation_layer,
    # shift_layer,
    # shear_layer,
    # zoom_layer,
    brightness_layer,
], augmentations_per_image=1)  # Apply all layers to each image


def train_and_evaluate(NLayer, n_folds, x_train, y_train, x_test, y_test, Batch_len, model_to_train, 
                       initializer1, initializer2, initializer10, Youtput, NEpochs, CustomCallback):
    
    accuracies = []
    train_accuracies = []
    
    # Configuração de diferentes folds
    for fold in range(n_folds):
        fold += 1
        print(f"\nFold {fold}\n")
            
        x_train_fold = x_train.reshape(-1, 64, 64, 3)
        y_train_fold = y_train    
        x_val_fold = x_test
        y_val_fold = y_test
        
        y_train_one_hot = to_categorical(y_train_fold, num_classes=5)
        y_val_one_hot = to_categorical(y_val_fold, num_classes=5)    
        
        train_dataset = tf.data.Dataset.from_tensor_slices((x_train_fold, y_train_one_hot))
        train_dataset = train_dataset.batch(Batch_len)
        train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

        # Modelo
        model = Sequential([
            Rescaling(scale=1./127.5, offset=-1, input_shape=(64, 64, 3)),
            model_to_train
        ])
        model.add(Flatten())
        
        Bias_ini = 'zeros'
        model.add(layers.Dense(N_neuronio, activation='relu', kernel_initializer=initializer1, bias_initializer=Bias_ini))
        
        # Configurar as Hidden layers (fora as que já estão presentes no Resnet50)
        for f in range(1, NLayer + 1):
            model.add(layers.Dense(N_neuronio, activation='relu', kernel_initializer=initializer2, bias_initializer=Bias_ini))
        
        model.add(layers.BatchNormalization())  
        model.add(tf.keras.layers.Dense(Youtput, activation='softmax', kernel_initializer=initializer10, bias_initializer=Bias_ini))   

        # Compilar o Modelo
        model.compile(optimizer=keras.optimizers.Adamax(learning_rate=LearningRate), 
                      loss='categorical_crossentropy',   
                      metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])  
        

        csv_file_path = f'logs/Template_ResNet50_only_brightness.csv'
        checkpoint = ModelCheckpoint("../Models/CrossValidation/ResNet50_64x64/Template_ResNet50_only_brightness.keras", 
                                     monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
        
        # Reduzir a learning rate se não houver melhoria na loss após x epochs (lembrar de deixar este valor sempre menor que a patience no early_stopping!!)
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)
        
        history = model.fit(train_dataset,
                            validation_data=(x_val_fold, y_val_one_hot), 
                            epochs=NEpochs, 
                            callbacks=[CustomCallback(csv_file_path), checkpoint, reduce_lr]) 

        
            
        # Gets training and validation accuracy results
        train_accuracy = history.history['accuracy']
        val_accuracy = history.history['val_accuracy']
        
        # Append results to lists
        train_accuracies.append(train_accuracy)
        accuracies.append(val_accuracy)

# Se só quisermos uma layer
if NLayer_ini == NLayermax:
    NLayer = NLayermax
    print("\nHidden Layer =", NLayer)
    train_and_evaluate(NLayer, n_folds, x_train, y_train, x_test, y_test, Batch_len, model_to_train, 
                       initializer1, initializer2, initializer10, Youtput, NEpochs, CustomCallback)
else:
    for NLayer in range(NLayer_ini, NLayermax + 1, deltaNLayer):
        print("\nHidden Layer =", NLayer)
        train_and_evaluate(NLayer, n_folds, x_train, y_train, x_test, y_test, Batch_len, model_to_train, 
                           initializer1, initializer2, initializer10, Youtput, NEpochs, CustomCallback)
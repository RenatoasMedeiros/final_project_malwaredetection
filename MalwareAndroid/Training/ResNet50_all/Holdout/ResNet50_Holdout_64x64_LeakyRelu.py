 
# # TREINO COM RESNET E DATASET IMAGEM 64x64x3 V2 HOLDOUT CALLBACKS


#Importar Bibliotecas
from keras.models import Sequential, load_model
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense, Rescaling, BatchNormalization
from keras.callbacks import ModelCheckpoint
from numpy import load
from tensorboard import version
import keras.backend as K

import csv

import os, datetime
import numpy as np
from keras.preprocessing import image

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, accuracy_score
from sklearn.model_selection import KFold

import sklearn.datasets
import gc


#Verificar versão do TF e ver GPU
import tensorflow as tf
print(tf.__version__)


print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

print("tensorboard.version.VERSION: ", version.VERSION)


directory='../Images/64x64'

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    directory,
    label_mode='int',
    seed=1337,
    image_size=(64,64),
    batch_size= None,
)


image_array = []
label_array = []

for element in train_ds:  
    
    image_array.append(np.asarray(element[0]))
    label_array.append(np.asarray(element[1]))

image_array = np.asarray(image_array)
label_array = np.asarray(label_array)

print(image_array.shape)
print(label_array.shape)

 
# ## TRAIN MODEL AND COMPARE WITH MODEL FOLD6


model_x_train, model_x_val, model_y_train, model_y_val = train_test_split(image_array, label_array,
                                                                          test_size=0.3, random_state=42)

model_x_val, model_x_test, model_y_val, model_y_test = train_test_split(model_x_val, model_y_val, test_size=0.5)

print('Data for training:')
print(model_x_train.shape)
print(model_y_train.shape)

print('Data for validation:')
print(model_x_val.shape)
print(model_y_val.shape)

print('Data for testing:')
print(model_x_test.shape)
print(model_y_test.shape)


gc.collect()

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True  # Allow GPU memory allocation to grow as needed
config.gpu_options.per_process_gpu_memory_fraction = 0.8  # Adjust the value as needed
tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))

with tf.device("/gpu:0"):
    # include_top=False => significa que somos nós a definir as camadas de inout e de output do modelo
    # pooling => Avg, Max, None
    pretrained_model=tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_tensor=None, 
                                                    input_shape=(64, 64, 3), pooling='Max', classes=5, classifier_activation="leaky_relu")

    # False: mantem os pesos carregados, nao sao alterados ao longo do treino
    for layer in pretrained_model.layers:
        layer.trainable=True # False 

    # definição do modelo
    model=Sequential([ 
        pretrained_model,
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(5, activation='softmax')
    ])

    model.compile(
        optimizer='adam',
        loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),
        metrics=['accuracy'])

    #Saved models path
    model_name = 'resnet50_holdout_64x64'
    save_path_best_model_holdout = '../Models/Holdout/ResNet50_64x64/' + model_name +'.hdf5'

    # CREATE CALLBACKS
    checkpoint_val_acc = tf.keras.callbacks.ModelCheckpoint(save_path_best_model_holdout, 
                                                    monitor='val_accuracy', verbose=0, 
                                                    save_best_only=True, mode='max')

    callbacks_list = [checkpoint_val_acc]

    # Fit data to model - uses x_train to train and validate
    model_history = model.fit(model_x_train,model_y_train,validation_data=(model_x_val,model_y_val), 
                              verbose=2, batch_size=40,callbacks=callbacks_list, epochs=50)

    #Frees space in GPU memory
    gc.collect()

    # Clear the TensorFlow session
    K.clear_session()   
    #load best model from fold
    best_model_holdout = load_model(save_path_best_model_holdout)

    # Generate generalization metrics - uses x_test to validate and predict
    model_scores = best_model_holdout.evaluate(model_x_val,model_y_val, verbose=0)

    # Calculate predicted labels for the test set
    model_y_pred = np.argmax(best_model_holdout.predict(model_x_test), axis=-1)

    # Calculate precision, recall, and F1 score
    model_accuracy = accuracy_score(model_y_test, model_y_pred)
    model_precision = precision_score(model_y_test, model_y_pred, average='macro')
    model_recall = recall_score(model_y_test, model_y_pred, average='macro')
    model_f1 = f1_score(model_y_test, model_y_pred, average='macro')
    model_cm = confusion_matrix(model_y_test, model_y_pred)
    
    


print('\n----------------------------------------------------------')
print(f'Score: Eval {model.metrics_names[0]} of {round(model_scores[0],5)}; Eval {model.metrics_names[1]} of {round(model_scores[1]*100,5)}%')
print(f'> Accuracy: {round(model_accuracy*100,5)}%')
print(f'> Precision: {round(model_precision*100,5)}%')
print(f'> Recall: {round(model_recall*100,5)}%')
print(f'> F1 Score: {round(model_f1*100,5)}%')

train_acc = model_history.history['accuracy']
train_loss = model_history.history['loss']
val_acc = model_history.history['val_accuracy']
val_loss = model_history.history['val_loss']

# Define the file path
csv_file_path = "resnet50_holdout_leakyrelu_model_his_tory.csv"

# Extract the history keys and values
history_data = {
    'epoch': range(1, len(train_acc) + 1),
    'train_accuracy': train_acc,
    'train_loss': train_loss,
    'val_accuracy': val_acc,
    'val_loss': val_loss
}

# Write the data to a CSV file
with open(csv_file_path, mode='w', newline='') as file:
    writer = csv.DictWriter(file, fieldnames=history_data.keys())
    writer.writeheader()
    for i in range(len(train_acc)):
        row_data = {key: history_data[key][i] for key in history_data.keys()}
        writer.writerow(row_data)

print("Model history data saved to:", csv_file_path)






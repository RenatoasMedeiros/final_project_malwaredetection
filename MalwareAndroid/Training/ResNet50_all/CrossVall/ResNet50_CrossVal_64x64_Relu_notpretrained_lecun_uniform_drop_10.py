 
# # TREINO COM RESNET E DATASET IMAGEM 64x64x3 V2 CROSS VALIDATION CALLBACKS


# Import libraries
from keras.models import Sequential, load_model
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense, Rescaling, BatchNormalization
from keras.callbacks import ModelCheckpoint
import csv
import os
import gc
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score
from keras.applications import ResNet50


# Print TensorFlow version
print(tf.__version__)

# Check if GPU is available
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))


# Directory containing images
directory = '../Images/64x64'

# Load images and labels
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    directory,
    label_mode='int',
    seed=1337,
    image_size=(64, 64),
    batch_size=None,
)

# Convert dataset to numpy arrays
image_array = []
label_array = []

for element in train_ds:
    image_array.append(np.asarray(element[0]))
    label_array.append(np.asarray(element[1]))

image_array = np.asarray(image_array)
label_array = np.asarray(label_array)

print(image_array.shape)
print(label_array.shape)


# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(image_array, label_array, test_size=0.1, random_state=42)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

 
# ## Training with ResNet50 and 64x64x3 image dataset - Cross-validation callbacks


# Garbage collection
gc.collect()

# Define per-fold score containers
acc_eval_per_fold = []
loss_eval_per_fold = []
acc_per_fold = []
loss_per_fold = []
precision_per_fold = []
recall_per_fold = []
f1_score_per_fold = []
cm_score_per_fold = []

# Initializers to use
weight_initializers = [
    #'glorot_normal', 
    # 'he_normal', 
    # 'lecun_normal', 
     'lecun_uniform', 
    # 'random_normal', 
]

# Path to save data to CSV
csv_file_path = "resnet50_crossval_history_model_not_pretrained_lecun_uniform_drop_10.csv"

# Open the CSV file in append mode, or create it if it doesn't exist
with open(csv_file_path, mode='a+', newline='') as file:
    writer = csv.writer(file)

    # Write the header only if the file is empty
    if os.path.getsize(self.csv_file_path) == 0:
        writer.writerow(['fold', 'initializer', 'epoch', 'train_accuracy', 'train_loss', 'val_accuracy', 'val_loss'])

    # Define the K-fold Cross Validator
    kfold = KFold(n_splits=5, shuffle=True)

    # Iterate over weight initializers
    for weight_initializer in weight_initializers:
        # Iterate over folds
        for fold_no in range(1, 6):
            # Define pretrained model
            model_to_train = ResNet50(include_top=False, weights=None, input_tensor=None,
                                        input_shape=(64, 64, 3), pooling='Max')

            # Manter a True para alterar os pesos
            for layer in model_to_train.layers:
                layer.trainable = True
                

            # Define model
            model = Sequential([
                model_to_train,
                Flatten(),
                Dropout(0.1),
                Dense(5, activation='softmax', kernel_initializer=weight_initializer)
            ])

            model.compile(
                optimizer='adamax',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

            learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
                initial_learning_rate=0.2,
                decay_steps=10000,
                decay_rate=0.96
                )

            # Create an optimizer with the learning rate schedule
            optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate_schedule)
            # Saved model path
            model_name = f'resnet50_crossval_64x64_{weight_initializer}_fold_{fold_no}'
            #save_path_model = f'../Models/CrossValidation/ResNet50_64x64/{model_name}_teste.hdf5'

            print('--------------------------------------------------------------------------------')
            print(f'Training for fold {fold_no} with weight initializer: {weight_initializer} ...')

            # Fit data to model
            history = model.fit(x_train, y_train, validation_split=0.2,
                                verbose=2, batch_size=16, epochs=50)

            # Write the data for each epoch to the CSV file
            with open(csv_file_path, mode='a', newline='') as file:
                writer = csv.writer(file)
                for epoch, (train_acc, train_loss, val_acc, val_loss) in enumerate(zip(history.history['accuracy'],
                                                                                       history.history['loss'],
                                                                                       history.history['val_accuracy'],
                                                                                       history.history['val_loss'])):
                    writer.writerow([fold_no, weight_initializer, epoch + 1, train_acc, train_loss, val_acc, val_loss])
                    file.flush()  # Flush the buffer after writing each row

            # Load best model
            #model = load_model(#save_path_model)

            # Evaluate model
            scores = model.evaluate(x_test, y_test, verbose=0)

            # Append scores to lists
            acc_eval_per_fold.append(scores[1] * 100)
            loss_eval_per_fold.append(scores[0])

            # Calculate and append other metrics like accuracy, precision, recall, F1 score, and confusion matrix
            y_pred = np.argmax(model.predict(x_test), axis=-1)
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='macro')
            recall = recall_score(y_test, y_pred, average='macro')
            f1 = f1_score(y_test, y_pred, average='macro')
            cm = confusion_matrix(y_test, y_pred)

            acc_per_fold.append(accuracy * 100)
            precision_per_fold.append(precision * 100)
            recall_per_fold.append(recall * 100)
            f1_score_per_fold.append(f1 * 100)
            cm_score_per_fold.append(cm)

# Print the precision, recall, and F1 score for each fold, as well as the average scores
print('Score per fold')
for i in range(len(acc_eval_per_fold)):
    print('------------------------------------------------------------------------')
    print(f'> Fold {i+1} - Eval Loss: {round(loss_eval_per_fold[i],5)} - Eval Accuracy: {round(acc_eval_per_fold[i],5)}% - Accuracy: {round(acc_per_fold[i],5)}% - Precision: {round(precision_per_fold[i],5)}% - Recall: {round(recall_per_fold[i],5)}% - F1 Score: {round(f1_score_per_fold[i],5)}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Evaluation Accuracy: {round(np.mean(acc_eval_per_fold),5)} (+- {round(np.std(acc_per_fold),5)})')
print(f'> Evaluation Loss: {round(np.mean(loss_eval_per_fold),5)}')
print(f'> Accuracy: {round(np.mean(acc_per_fold),5)} (+- {round(np.std(acc_per_fold),5)})')
print(f'> Precision: {round(np.mean(precision_per_fold),5)} (+- {round(np.std(precision_per_fold),5)})')
print(f'> Recall: {round(np.mean(recall_per_fold),5)} (+- {round(np.std(recall_per_fold),5)})')
print(f'> F1 Score: {round(np.mean(f1_score_per_fold),5)} (+- {round(np.std(f1_score_per_fold),5)})')
print('------------------------------------------------------------------------')

print("History data for all folds saved to:", csv_file_path)

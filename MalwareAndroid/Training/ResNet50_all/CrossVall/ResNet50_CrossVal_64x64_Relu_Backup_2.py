 
# # TREINO COM RESNET E DATASET IMAGEM 64x64x3 V2 CROSS VALIDATION CALLBACKS


# Import libraries
from keras.models import Sequential, load_model
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense, Rescaling, BatchNormalization
from keras.callbacks import ModelCheckpoint
import gc
import csv
import os
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score
from keras.applications import ResNet50


# Print TensorFlow version
print(tf.__version__)

# Check if GPU is available
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))


# Directory containing images
directory = '../Images/64x64'

# Load images and labels
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    directory,
    label_mode='int',
    seed=1337,
    image_size=(64, 64),
    batch_size=None,
)

# Convert dataset to numpy arrays
image_array = []
label_array = []

for element in train_ds:
    image_array.append(np.asarray(element[0]))
    label_array.append(np.asarray(element[1]))

image_array = np.asarray(image_array)
label_array = np.asarray(label_array)

print(image_array.shape)
print(label_array.shape)


# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(image_array, label_array, test_size=0.1, random_state=42)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

 
# ## Training with ResNet50 and 64x64x3 image dataset - Cross-validation callbacks


# Garbage collection
gc.collect()

# Define per-fold score containers
acc_eval_per_fold = []
loss_eval_per_fold = []
acc_per_fold = []
loss_per_fold = []
precision_per_fold = []
recall_per_fold = []
f1_score_per_fold = []
cm_score_per_fold = []

# Initializers to use
weight_initializers = [
    'constant', 
    'glorot_normal', 
    'glorot_uniform', 
    'he_normal', 
    'he_uniform',
    'identity', 
    'lecun_normal', 
    'lecun_uniform', 
    'ones', 
    'orthogonal',
    'random_normal', 
    'random_uniform', 
    'truncated_normal', 
    'variance_scaling', 
    'zeros'
]

# Path to save data to CSV
csv_file_path = "resnet50_crossval_history_model_weights.csv"

# Open the CSV file in append mode
with open(csv_file_path, mode='a', newline='') as file:
    writer = csv.writer(file)

    # Write the header only if the file is empty
    if os.path.getsize(self.csv_file_path) == 0:
        writer.writerow(['fold', 'initializer', 'epoch', 'train_accuracy', 'train_loss', 'val_accuracy', 'val_loss'])

    # Find the last recorded fold and initializer
    last_recorded_fold = 0
    last_recorded_initializer = ''
    with open(csv_file_path, mode='r') as read_file:
        csv_reader = csv.reader(read_file)
        for row in csv_reader:
            if row[0] != 'fold':
                last_recorded_fold = max(last_recorded_fold, int(row[0]))
                if row[0] == str(last_recorded_fold):
                    last_recorded_initializer = row[1]

    # Define the K-fold Cross Validator
    kfold = KFold(n_splits=5, shuffle=True)

    # Resume training from the next fold and initializer
    for weight_initializer in weight_initializers:
        # Check if the current initializer is before the last recorded initializer
        if weight_initializer < last_recorded_initializer:
            continue
        fold_no_start = last_recorded_fold + 1 if weight_initializer == last_recorded_initializer else 1

        for fold_no in range(fold_no_start, 6):
            # Define pretrained model
            pretrained_model = ResNet50(include_top=False, weights='imagenet', input_tensor=None,
                                        input_shape=(64, 64, 3), pooling='Max')

            # Manter a True para alterar os pesos
            for layer in pretrained_model.layers:
                layer.trainable = True

            # Define model
            model = Sequential([
                pretrained_model,
                Flatten(),
                Dense(5, activation='softmax', kernel_initializer=weight_initializer)
            ])

            model.compile(
                optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

            # Saved model path
            model_name = f'resnet50_crossval_64x64_{weight_initializer}_fold_{fold_no}'
            save_path_best_model = f'../Models/CrossValidation/ResNet50_64x64/{model_name}.hdf5'

            print('--------------------------------------------------------------------------------')
            print(f'Training for fold {fold_no} with weight initializer: {weight_initializer} ...')

            # Define starting epoch
            start_epoch = 0
            if fold_no == last_recorded_fold and weight_initializer == last_recorded_initializer:
                # Check the last recorded epoch for this fold and initializer
                with open(csv_file_path, mode='r') as read_file:
                    csv_reader = csv.reader(read_file)
                    for row in csv_reader:
                        if row[0] == str(fold_no) and row[1] == weight_initializer:
                            start_epoch = max(start_epoch, int(row[2]))

            # Fit data to model starting from the determined epoch
            history = model.fit(x_train, y_train, validation_split=0.2,
                                verbose=2, batch_size=16, epochs=50, initial_epoch=start_epoch)

            # Write the data for each epoch to the CSV file
            with open(csv_file_path, mode='a', newline='') as file:
                writer = csv.writer(file)
                for epoch, (train_acc, train_loss, val_acc, val_loss) in enumerate(zip(history.history['accuracy'],
                                                                                       history.history['loss'],
                                                                                       history.history['val_accuracy'],
                                                                                       history.history['val_loss'])):
                    writer.writerow([fold_no, weight_initializer, epoch + start_epoch + 1, train_acc, train_loss, val_acc, val_loss])
                    file.flush()  # Flush the buffer after writing each row

            # Load best model
            best_model = load_model(save_path_best_model)

            # Evaluate model
            scores = best_model.evaluate(x_test, y_test, verbose=0)

            # Append scores to lists
            acc_eval_per_fold.append(scores[1] * 100)
            loss_eval_per_fold.append(scores[0])

            # Calculate and append other metrics like accuracy, precision, recall, F1 score, and confusion matrix
            y_pred = np.argmax(best_model.predict(x_test), axis=-1)
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='macro')
            recall = recall_score(y_test, y_pred, average='macro')
            f1 = f1_score(y_test, y_pred, average='macro')
            cm = confusion_matrix(y_test, y_pred)

            acc_per_fold.append(accuracy * 100)
            precision_per_fold.append(precision * 100)
            recall_per_fold.append(recall * 100)
            f1_score_per_fold.append(f1 * 100)
            cm_score_per_fold.append(cm)

# Print the precision, recall, and F1 score for each fold, as well as the average scores
print('Score per fold')
for i in range(len(acc_eval_per_fold)):
    print('------------------------------------------------------------------------')
    print(f'> Fold {i+1} - Eval Loss: {round(loss_eval_per_fold[i],5)} - Eval Accuracy: {round(acc_eval_per_fold[i],5)}% - Accuracy: {round(acc_per_fold[i],5)}% - Precision: {round(precision_per_fold[i],5)}% - Recall: {round(recall_per_fold[i],5)}% - F1 Score: {round(f1_score_per_fold[i],5)}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Evaluation Accuracy: {round(np.mean(acc_eval_per_fold),5)} (+- {round(np.std(acc_per_fold),5)})')
print(f'> Evaluation Loss: {round(np.mean(loss_eval_per_fold),5)}')
print(f'> Accuracy: {round(np.mean(acc_per_fold),5)} (+- {round(np.std(acc_per_fold),5)})')
print(f'> Precision: {round(np.mean(precision_per_fold),5)} (+- {round(np.std(precision_per_fold),5)})')
print(f'> Recall: {round(np.mean(recall_per_fold),5)} (+- {round(np.std(recall_per_fold),5)})')
print(f'> F1 Score: {round(np.mean(f1_score_per_fold),5)} (+- {round(np.std(f1_score_per_fold),5)})')
print('------------------------------------------------------------------------')

print("History data for all folds saved to:", csv_file_path)
